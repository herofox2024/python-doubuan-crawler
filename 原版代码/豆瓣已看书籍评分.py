import requests
import pandas as pd
from bs4 import BeautifulSoup
import re
import time
import json
import os
from typing import Optional, Tuple, List
from tqdm import tqdm

def get_book_details(book_url, headers):
    """è·å–ä¹¦ç±è¯¦ç»†ä¿¡æ¯ï¼šä½œè€…å’Œå‡ºç‰ˆå¹´æœˆ"""
    try:
        time.sleep(1)  # æ·»åŠ å»¶æ—¶é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        response = requests.get(book_url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è·å–ä½œè€…ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼Œå…¼å®¹å¤šç§æ ¼å¼ï¼‰
        author = "æœªçŸ¥ä½œè€…"
        info_elem = soup.select_one('#info')
        if info_elem:
            info_html = str(info_elem)
            info_text = info_elem.get_text(separator=' ', strip=True)
            # 1. å…ˆå°è¯• a æ ‡ç­¾
            author_element = info_elem.select_one('a[href*="/author/"]')
            if author_element:
                author = author_element.text.strip()
            else:
                # 2. å°è¯•æ­£åˆ™æå–"ä½œè€…:"åå†…å®¹ï¼ˆä¸ä¾èµ–aæ ‡ç­¾ï¼‰
                author_match = re.search(r'ä½œè€…[:ï¼š]?\s*([^/\n]+)', info_text)
                if author_match:
                    author = author_match.group(1).strip()
        
        # è·å–å‡ºç‰ˆå¹´æœˆ
        publish_date = "æœªçŸ¥"
        if info_elem:
            info_text = info_elem.text
            # æŸ¥æ‰¾å‡ºç‰ˆå¹´æœˆï¼ˆæ ¼å¼å¦‚ï¼š2023-1ã€2023å¹´1æœˆç­‰ï¼‰
            date_match = re.search(r'å‡ºç‰ˆå¹´:?\s*(\d{4}[-å¹´]\d{1,2}[æœˆ]?)', info_text)
            if date_match:
                publish_date = date_match.group(1)
        
        return author, publish_date
    except Exception as e:
        print(f"è·å–ä¹¦ç±è¯¦æƒ…å¤±è´¥ {book_url}: {e}")
        return "è·å–å¤±è´¥", "è·å–å¤±è´¥"

def extract_rating_from_class(item):
    """ä»è±†ç“£é¡µé¢çš„CSS classä¸­æå–è¯„åˆ†ï¼ˆè¿”å›æ ¼å¼ï¼š'æ•°å­—æ˜Ÿ'ï¼Œå¦‚'4æ˜Ÿ'ï¼‰"""
    rating_span = item.select_one('span[class*="rating"]')
    if rating_span:
        class_names = rating_span.get('class', [])
        for class_name in class_names:
            # æŸ¥æ‰¾ç±»ä¼¼ rating4-t çš„æ ¼å¼
            match = re.search(r'rating(\d+)', class_name)
            if match:
                rating_num = int(match.group(1))
                return f"{rating_num}æ˜Ÿ"
    return None

def extract_review_content(item):
    """æå–ä¹¦è¯„å†…å®¹"""
    # ä¼˜å…ˆå°è¯• list æ¨¡å¼çš„ä¹¦è¯„æå–
    comment_elem = item.select_one('p.comment.comment-item')
    if comment_elem:
        return comment_elem.get_text().strip()
    
    # å¤‡ç”¨ï¼šæ ‡å‡†æ¨¡å¼çš„ä¹¦è¯„æå–
    comment_elem = item.select_one('p.comment')
    if comment_elem:
        return comment_elem.get_text().strip()
    
    return None

def load_config():
    """ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½Cookieé…ç½®"""
    COOKIE = os.getenv('DOUBAN_COOKIE', '_vwo_uuid_v2=D381335B8236F6CFD6DAF37C9E7F28ACE|0aac072639388569f98aa4438dc96fb9; push_doumail_num=0; bid=3B9Ip4LehUU; ll="118172"; __yadk_uid=Cy7V1K3tvokSaiMEo1CDXPe64A81E3x9; _pk_id.100001.8cb4=47ccc5d04603212d.1750383490.; push_noty_num=0; _ga_RXNMP372GL=GS2.1.s1754553560$o5$g0$t1754553560$j60$l0$h0; _ga=GA1.2.1311955456.1739170185; _ga_Y4GN1R87RG=GS2.1.s1754636160$o4$g0$t1754636167$j53$l0$h0; viewed="36328704_37335235_26667592_35763332_36710597_37008516_37009860"; loc-last-index-location-id="118172"; __utmz=30149280.1755162972.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1755218706%2C%22https%3A%2F%2Fbook.douban.com%2Fpeople%2Fherofox%2Fcollect%22%5D; _pk_ses.100001.8cb4=1; ap_v=0,6.0; __utma=30149280.1311955456.1739170185.1755162972.1755218708.2; __utmc=30149280; __utmt=1; dbcl2="205601419:fz2NXixmqVo"; ck=7x0z; frodotk_db="3796152bc02ac10ba6c23f8340804321"; __utmv=30149280.20560; _TDID_CK=1755218913790; 6333762c95037d16=%2FkvzbE%2BwFGbVkUGOwQkNAcz9NIulIyZo2JcJxXHvOWext%2F5jUi66P9hqKqUZx%2B1cSCA6VoJ8hFjb1tEAClGg%2BhLsORhPlu3NqsMn%2FNCw7QlSTC99PT%2BxrIQZmlpLUSedyZIxoAnUCfPllTVnbsPUzfxmGt6Y1LpuGtWGthgUMuDBxFBEdH%2FeVAPHBKSgytAT4Oc60MutPtj25K0cENaOLJisCFizp4oqvgMqvnbC%2B2qr%2BYCm%2FKpyOLJqyf8mBrumrSWtbvZaZk2vmGHAw6W7mojAWUbTaI%2BqhP5%2BsYveHCorNunECyTR5Q%3D%3D; __utmb=30149280.6.10.1755218708')
    
    if not COOKIE:
        # å°è¯•ä»æ–‡ä»¶è¯»å–
        try:
            with open('config.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
                COOKIE = config.get('cookie', '')
        except FileNotFoundError:
            print("âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°Cookieé…ç½®")
            print("ğŸ“ è±†ç“£ä¸ªäººæ”¶è—é¡µé¢éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®ï¼Œä»…æä¾›ç”¨æˆ·åæ— æ³•æ­£å¸¸ä½¿ç”¨")
            print("ğŸ”§ è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è·å–Cookieï¼š")
            print("   1. åœ¨æµè§ˆå™¨ä¸­ç™»å½•è±†ç“£")
            print("   2. è®¿é—®ä½ çš„æ”¶è—é¡µé¢")
            print("   3. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·")
            print("   4. åœ¨Networkæ ‡ç­¾é¡µä¸­æ‰¾åˆ°é¡µé¢è¯·æ±‚")
            print("   5. å¤åˆ¶è¯·æ±‚å¤´ä¸­çš„Cookieå€¼")
            print("   6. è®¾ç½®ç¯å¢ƒå˜é‡DOUBAN_COOKIEæˆ–åˆ›å»ºconfig.jsonæ–‡ä»¶")
    
    return COOKIE

def get_user_input():
    """è·å–ç”¨æˆ·è¾“å…¥çš„è±†ç“£ç”¨æˆ·å"""
    print("\nğŸ”¸ è¯·è¾“å…¥è±†ç“£ç”¨æˆ·åï¼ˆURLä¸­people/åé¢çš„éƒ¨åˆ†ï¼‰")
    print("   ä¾‹å¦‚ï¼šhttps://book.douban.com/people/your_username/collect ä¸­çš„ your_username")
    user_id = input("è±†ç“£ç”¨æˆ·å: ").strip()
    
    if not user_id:
        print("âŒ ç”¨æˆ·åä¸èƒ½ä¸ºç©ºï¼")
        return None
    
    return user_id

def export_douban_books_with_reviews(user_id: str, max_pages: int = None):
    COOKIE = load_config()
    
    if not COOKIE:
        print("âŒ é”™è¯¯ï¼šæœªé…ç½®Cookieï¼Œæ— æ³•ç»§ç»­")
        print("ğŸ’¡ è±†ç“£éœ€è¦ç™»å½•çŠ¶æ€æ‰èƒ½è®¿é—®ä¸ªäººæ”¶è—é¡µé¢")
        print("ğŸ“– è¯·å‚è€ƒä¸Šè¿°è¯´æ˜è·å–å¹¶é…ç½®Cookie")
        return
    
    headers = {
        'Cookie': COOKIE, 
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    books_with_reviews = []
    all_books = []
    page = 0
    failed_pages = []
    max_retries = 3
    base_delay = 2
    max_delay = 10
    
    print(f"å¼€å§‹çˆ¬å–ç”¨æˆ· {user_id} çš„è±†ç“£ä¹¦ç±æ•°æ®...")
    if max_pages:
        print(f"è®¾ç½®æœ€å¤§é¡µæ•°é™åˆ¶: {max_pages}")
    else:
        print("å°†çˆ¬å–æ‰€æœ‰é¡µé¢ç›´åˆ°æ²¡æœ‰æ•°æ®")
    
    # æ€»çš„å¯¼å‡ºæ–‡ä»¶è¿›åº¦æ¡ - ä½äºæœ€é¡¶éƒ¨
    total_steps = 6  # æ€»å…±6ä¸ªä¸»è¦æ­¥éª¤ï¼šæ•°æ®çˆ¬å–ã€å®Œæ•´æ¸…å•å¯¼å‡ºã€ç»Ÿè®¡å¯¼å‡ºã€ä¹¦è¯„å¯¼å‡ºã€æŒ‰è¯„åˆ†åˆ†ç»„å¯¼å‡ºã€å®Œæˆ
    overall_progress = tqdm(total=total_steps, desc="æ€»å¯¼å‡ºè¿›åº¦", unit="æ­¥éª¤", position=0, 
                           bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]')
    
    # åˆ›å»ºé¡µé¢è¿›åº¦æ¡
    page_progress = tqdm(desc="é¡µé¢è¿›åº¦", unit="é¡µ", position=1)
    
    while True:
        # å¦‚æœè®¾ç½®äº†æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œåˆ™æ£€æŸ¥
        if max_pages and page >= max_pages:
            page_progress.close()
            overall_progress.set_description("æ•°æ®çˆ¬å–å®Œæˆï¼Œè¾¾åˆ°é¡µæ•°é™åˆ¶")
            overall_progress.update(1)  # å®Œæˆæ•°æ®çˆ¬å–æ­¥éª¤
            print(f"å·²è¾¾åˆ°è®¾ç½®çš„æœ€å¤§é¡µæ•°é™åˆ¶ ({max_pages})ï¼Œåœæ­¢çˆ¬å–")
            break
        # è±†ç“£ä¹¦ç±æ”¶è—é¡µé¢æ¯é¡µæ˜¾ç¤º15æœ¬ä¹¦
        url = f'https://book.douban.com/people/{user_id}/collect?start={page*15}'
        success = False
        
        for attempt in range(max_retries):
            try:
                page_progress.set_description(f"æ­£åœ¨è¯·æ±‚ç¬¬{page+1}é¡µ (å°è¯• {attempt+1}/{max_retries})")
                # æŒ‡æ•°é€€é¿å»¶æ—¶ç­–ç•¥
                delay = min(base_delay * (2 ** attempt), max_delay)
                time.sleep(delay)
                # ç§»é™¤Accept-Encodingæ¥è®©æœåŠ¡å™¨è¿”å›æœªå‹ç¼©çš„å†…å®¹
                temp_headers = headers.copy()
                temp_headers.pop('Accept-Encoding', None)
                res = requests.get(url, headers=temp_headers, timeout=30)
                
                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯é¡µé¢
                if 'sec.douban.com' in res.url or 'ç¦æ­¢è®¿é—®' in res.text or res.status_code == 403:
                    page_progress.close()
                    overall_progress.close()
                    print("\né‡åˆ°åçˆ¬è™«éªŒè¯ï¼ŒCookieå¯èƒ½å·²è¿‡æœŸæˆ–éœ€è¦äººå·¥éªŒè¯")
                    print(f"å½“å‰URL: {res.url}")
                    print("è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®è±†ç“£å¹¶å®ŒæˆéªŒè¯ï¼Œç„¶åæ›´æ–°Cookie")
                    return
                
                res.raise_for_status()
                success = True
                break
                
            except requests.exceptions.RequestException as e:
                page_progress.set_description(f"ç¬¬{page+1}é¡µè¯·æ±‚å¤±è´¥ (å°è¯• {attempt+1}): {e}")
                if attempt < max_retries - 1:
                    error_delay = min(base_delay * (3 ** attempt), max_delay)
                    time.sleep(error_delay)
                else:
                    failed_pages.append(page)
                    page_progress.write(f"ç¬¬{page+1}é¡µé‡è¯•{max_retries}æ¬¡åä»ç„¶å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µ")
        
        if not success:
            page += 1
            if page > (max_pages * 2 if max_pages else 100):  # é˜²æ­¢æ— é™å¾ªç¯
                page_progress.close()
                overall_progress.close()
                print("\nè¾¾åˆ°æœ€å¤§é‡è¯•é¡µæ•°ï¼Œåœæ­¢çˆ¬å–")
                break
            continue
        
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # æŸ¥æ‰¾ä¹¦ç±æ¡ç›®
        items = soup.select('li.subject-item')
        page_progress.set_description(f"ç¬¬{page+1}é¡µæ‰¾åˆ° {len(items)} æœ¬ä¹¦ç±")
        
        if not items:
            page_progress.close()
            overall_progress.set_description("æ•°æ®çˆ¬å–å®Œæˆï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
            overall_progress.update(1)  # å®Œæˆæ•°æ®çˆ¬å–æ­¥éª¤
            print(f"\nç¬¬{page+1}é¡µæ²¡æœ‰æ‰¾åˆ°ä¹¦ç±æ¡ç›®ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
            break
        
        page_books_count = 0
        
        # åˆ›å»ºä¹¦ç±å¤„ç†è¿›åº¦æ¡
        books_progress = tqdm(items, desc=f"ç¬¬{page+1}é¡µä¹¦ç±", unit="æœ¬", position=2, leave=False)
        
        for i, item in enumerate(books_progress, 1):
            try:
                # è·å–ä¹¦ç±æ ‡é¢˜å’Œé“¾æ¥
                title_element = item.select_one('h2 a')
                if not title_element:
                    books_progress.write(f"  ä¹¦ç±{i}: æ— æ³•è·å–æ ‡é¢˜å…ƒç´ ")
                    continue
                    
                title = title_element.get('title', '').strip()
                if not title:
                    title = title_element.get_text().strip()
                    
                link = title_element.get('href', '').strip()
                
                if not title or title == 'æœªçŸ¥ä¹¦å':
                    books_progress.write(f"  è­¦å‘Š: ä¹¦ç±{i}æ ‡é¢˜ä¸ºç©ºæˆ–æœªçŸ¥: {link}")
                
                books_progress.set_description(f"ç¬¬{page+1}é¡µä¹¦ç±: ã€Š{title[:20]}...ã€‹")
                
                # ä»å½“å‰é¡µé¢æå–ä½œè€…å’Œå‡ºç‰ˆä¿¡æ¯
                author, publish_date = extract_book_info_from_page(item)
                
                
                # ä»CSS classä¸­æå–è¯„åˆ†
                rating = extract_rating_from_class(item)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¯„åˆ†ï¼Œå°è¯•å…¶ä»–æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰
                if rating is None:
                    rating_tag = item.select_one('.rating_nums')
                    if rating_tag:
                        try:
                            rating = f"{rating_tag.text.strip()}åˆ†"
                        except ValueError:
                            rating = "æœªè¯„åˆ†"
                    else:
                        rating = "æœªè¯„åˆ†"
                
                # æå–ä¹¦è¯„å†…å®¹
                review = extract_review_content(item)
                
                # è·å–è¯„åˆ†æ—¥æœŸ
                date_elem = item.select_one('.date')
                date = date_elem.get_text().strip() if date_elem else 'æœªçŸ¥æ—¥æœŸ'
                
                # æ·»åŠ åˆ°æ‰€æœ‰ä¹¦ç±åˆ—è¡¨
                all_books.append([title, author, publish_date, link, rating, review or 'æ— ä¹¦è¯„', date])
                page_books_count += 1
                
                # å¦‚æœæœ‰è¯„åˆ†ä¸”æœ‰ä¹¦è¯„ï¼Œæ·»åŠ åˆ°ä¹¦è¯„åˆ—è¡¨
                if rating and rating != "æœªè¯„åˆ†" and review:
                    books_with_reviews.append([title, author, publish_date, link, rating, review, date])
                    
            except Exception as e:
                books_progress.write(f"    å¤„ç†ç¬¬{i}æœ¬ä¹¦æ—¶å‡ºé”™: {e}")
                continue
        
        # å…³é—­ä¹¦ç±è¿›åº¦æ¡
        books_progress.close()
        
        page_progress.update(1)
        page_progress.write(f"ç¬¬{page+1}é¡µå¤„ç†å®Œæˆï¼Œæœ¬é¡µæ‰¾åˆ°{page_books_count}æœ¬ä¹¦ç±ï¼Œç´¯è®¡{len(all_books)}æœ¬")
        page_progress.write(f"ç›®å‰æ‰¾åˆ°{len(books_with_reviews)}æœ¬æœ‰ä¹¦è¯„çš„ä¹¦ç±")
        
        page += 1

    # å…³é—­é¡µé¢è¿›åº¦æ¡
    page_progress.close()
    
    # æ›´æ–°æ€»è¿›åº¦ï¼šæ•°æ®çˆ¬å–å®Œæˆ
    if not overall_progress.n:  # å¦‚æœè¿˜æ²¡æœ‰æ›´æ–°è¿‡æ•°æ®çˆ¬å–æ­¥éª¤
        overall_progress.set_description("æ•°æ®çˆ¬å–å®Œæˆ")
        overall_progress.update(1)
    
    print(f"\nçˆ¬å–å®Œæˆï¼")
    print(f"æ€»å…±å¤„ç†äº†{page}é¡µ")
    print(f"å¤±è´¥çš„é¡µé¢: {failed_pages}")
    print(f"æ‰¾åˆ°{len(all_books)}æœ¬ä¹¦ç±")
    print(f"å…¶ä¸­{len(books_with_reviews)}æœ¬æœ‰ä¹¦è¯„")

    # å¯¼å‡ºæ‰€æœ‰ä¹¦ç±ï¼ˆåŒ…å«ä¹¦è¯„ä¿¡æ¯ï¼‰
    if all_books:
        # æ›´æ–°æ€»è¿›åº¦ï¼šå¼€å§‹å¯¼å‡ºå®Œæ•´æ¸…å•
        overall_progress.set_description("æ­£åœ¨å¯¼å‡ºå®Œæ•´ä¹¦ç±æ¸…å•")
        overall_progress.update(1)
        
        df_all = pd.DataFrame(all_books, columns=['ä¹¦å', 'ä½œè€…', 'å‡ºç‰ˆå¹´æœˆ', 'é“¾æ¥', 'è¯„åˆ†', 'ä¹¦è¯„å†…å®¹', 'è¯„åˆ†æ—¥æœŸ'])
        df_all.to_excel('è±†ç“£ä¹¦ç±å®Œæ•´æ¸…å•.xlsx', index=False)
        print(f"æˆåŠŸå¯¼å‡º{len(all_books)}æœ¬ä¹¦ç±å®Œæ•´æ•°æ®åˆ° 'è±†ç“£ä¹¦ç±å®Œæ•´æ¸…å•.xlsx'")
        
        # æ›´æ–°æ€»è¿›åº¦ï¼šå¼€å§‹å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯
        overall_progress.set_description("æ­£åœ¨å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯")
        overall_progress.update(1)
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'æ€»ä¹¦ç±æ•°': len(all_books),
            'æœ‰ä¹¦è¯„æ•°': len(books_with_reviews),
            'å¤„ç†é¡µæ•°': page,
            'å¤±è´¥é¡µæ•°': len(failed_pages),
            'å¤±è´¥é¡µé¢åˆ—è¡¨': failed_pages
        }
        with open('çˆ¬å–ç»Ÿè®¡.json', 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ° 'çˆ¬å–ç»Ÿè®¡.json'")
        
        # æ›´æ–°æ€»è¿›åº¦ï¼šå¼€å§‹å¯¼å‡ºä¹¦è¯„
        overall_progress.set_description("æ­£åœ¨å¯¼å‡ºä¹¦è¯„æ•°æ®")
        overall_progress.update(1)
        
        # å¯¼å‡ºæœ‰ä¹¦è¯„çš„ä¹¦ç±
        export_reviews_only(books_with_reviews, overall_progress)
    else:
        overall_progress.close()
        print("æœªæ‰¾åˆ°ä»»ä½•ä¹¦ç±æ•°æ®")
        return


def export_reviews_only(books_with_reviews, overall_progress=None):
    """å°è£…çš„ä¹¦è¯„å¯¼å‡ºåŠŸèƒ½"""
    if books_with_reviews:
        df_reviews = pd.DataFrame(books_with_reviews, columns=['ä¹¦å', 'ä½œè€…', 'å‡ºç‰ˆå¹´æœˆ', 'é“¾æ¥', 'è¯„åˆ†', 'ä¹¦è¯„å†…å®¹', 'è¯„åˆ†æ—¥æœŸ'])
        df_reviews.to_excel('è±†ç“£ä¹¦è¯„æ¸…å•.xlsx', index=False)
        print(f"æˆåŠŸå¯¼å‡º{len(books_with_reviews)}æœ¬æœ‰ä¹¦è¯„çš„ä¹¦ç±æ•°æ®åˆ° 'è±†ç“£ä¹¦è¯„æ¸…å•.xlsx'")
        
        # æ›´æ–°æ€»è¿›åº¦ï¼šå¼€å§‹æŒ‰è¯„åˆ†åˆ†ç»„å¯¼å‡º
        if overall_progress:
            overall_progress.set_description("æ­£åœ¨æŒ‰è¯„åˆ†åˆ†ç»„å¯¼å‡º")
            overall_progress.update(1)
        
        # æŒ‰è¯„åˆ†åˆ†ç»„å¯¼å‡º
        for rating in sorted(df_reviews['è¯„åˆ†'].unique()):
            rating_books = df_reviews[df_reviews['è¯„åˆ†'] == rating]
            filename = f'è±†ç“£{rating}ä¹¦è¯„.xlsx'
            rating_books.to_excel(filename, index=False)
            print(f"å¯¼å‡º{rating}ä¹¦è¯„ {len(rating_books)}æœ¬åˆ° '{filename}'")
        
        # å®Œæˆæ‰€æœ‰å¯¼å‡ºï¼Œå…³é—­æ€»è¿›åº¦æ¡
        if overall_progress:
            overall_progress.set_description("æ‰€æœ‰æ–‡ä»¶å¯¼å‡ºå®Œæˆï¼")
            overall_progress.update(1)
            overall_progress.close()
    else:
        print("æœªæ‰¾åˆ°æœ‰ä¹¦è¯„çš„ä¹¦ç±")
        if overall_progress:
            overall_progress.close()

def extract_book_info_from_page(item) -> Tuple[str, str]:
    """ä»é¡µé¢å…ƒç´ ä¸­æå–ä¹¦ç±ä¿¡æ¯"""
    author = "æœªçŸ¥ä½œè€…"
    publish_date = "æœªçŸ¥"
    
    pub_elem = item.select_one('.pub')
    if pub_elem:
        pub_text = pub_elem.get_text().strip()
        # è§£æç±»ä¼¼ "é©¬ä¼¯åº¸ / æ¹–å—æ–‡è‰ºå‡ºç‰ˆç¤¾ / 2025-6 / 48.00å…ƒ" çš„æ ¼å¼
        parts = pub_text.split(' / ')
        
        # æå–ä½œè€…ï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰
        if len(parts) >= 1:
            author = parts[0].strip()
        
        # æ™ºèƒ½æå–å‡ºç‰ˆå¹´æœˆï¼ˆå¯»æ‰¾åŒ…å«å¹´ä»½çš„éƒ¨åˆ†ï¼‰
        for part in parts:
            part = part.strip()
            # åŒ¹é…å¹´æœˆæ ¼å¼ï¼šYYYY-MM, YYYY/MM, YYYYå¹´MMæœˆç­‰
            if any(pattern in part for pattern in ['20', '19']) and any(char in part for char in ['-', '/', 'å¹´']):
                # è¿›ä¸€æ­¥éªŒè¯æ˜¯å¦ä¸ºæ—¥æœŸæ ¼å¼
                if re.search(r'\b(19|20)\d{2}[-å¹´/]\d{1,2}[æœˆ]?', part) or re.search(r'\b(19|20)\d{2}[-/]\d{1,2}[-/]\d{1,2}', part):
                    publish_date = part
                    break
                elif re.search(r'\b(19|20)\d{2}', part) and len(part) <= 10:  # ç®€å•å¹´ä»½åˆ¤æ–­
                    publish_date = part
                    break
    
    return author, publish_date

if __name__ == '__main__':
    # è·å–ç”¨æˆ·è¾“å…¥çš„è±†ç“£ç”¨æˆ·å
    user_id = get_user_input()
    if not user_id:
        print("ç¨‹åºé€€å‡º")
        exit(1)
    
    # å¯ä»¥é€šè¿‡å‚æ•°æ§åˆ¶çˆ¬å–é¡µæ•°ï¼ŒNoneè¡¨ç¤ºçˆ¬å–æ‰€æœ‰é¡µé¢
    pages_env = os.getenv('MAX_PAGES', '')
    if pages_env and pages_env.isdigit():
        pages = int(pages_env)
    else:
        pages = None  # çˆ¬å–æ‰€æœ‰é¡µé¢
    
    print(f"å‡†å¤‡å¼€å§‹çˆ¬å–ç”¨æˆ· {user_id} çš„æ•°æ®ï¼Œé¡µæ•°è®¾ç½®: {'æ‰€æœ‰é¡µé¢' if pages is None else f'{pages}é¡µ'}")
    export_douban_books_with_reviews(user_id, max_pages=pages)